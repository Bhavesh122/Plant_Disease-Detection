{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11265e73-f858-406e-9dd2-e527c5e37c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Folders: ['grayscale', '.DS_Store', 'segmented', 'color']\n",
      "Found 43456 images belonging to 38 classes.\n",
      "Found 10849 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Enable Apple Metal GPU acceleration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Image Parameters\n",
    "img_size = 224  # Target image size\n",
    "batch_size = 32  # Batch size for model training\n",
    "\n",
    "# Dataset Path\n",
    "base_dir = '/Users/bhaveshreddy/Desktop/Capstone Project/plantvillage dataset'\n",
    "\n",
    "# Check if the dataset folders exist\n",
    "print(\"Dataset Folders:\", os.listdir(base_dir))\n",
    "\n",
    "# Image Data Generators\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "# Train Generator\n",
    "train_generator = data_gen.flow_from_directory(\n",
    "    os.path.join(base_dir, \"color\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Validation Generator\n",
    "validation_generator = data_gen.flow_from_directory(\n",
    "    os.path.join(base_dir, \"color\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84c01cf-cbba-4612-b6ab-26888ced229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1983s\u001b[0m 1s/step - accuracy: 0.7197 - loss: 1.0323 - val_accuracy: 0.9307 - val_loss: 0.2208\n",
      "Epoch 2/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1964s\u001b[0m 1s/step - accuracy: 0.9136 - loss: 0.2676 - val_accuracy: 0.9434 - val_loss: 0.1676\n",
      "Epoch 3/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1983s\u001b[0m 1s/step - accuracy: 0.9317 - loss: 0.2087 - val_accuracy: 0.9465 - val_loss: 0.1562\n",
      "Epoch 4/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2083s\u001b[0m 2s/step - accuracy: 0.9393 - loss: 0.1767 - val_accuracy: 0.9471 - val_loss: 0.1544\n",
      "Epoch 5/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1968s\u001b[0m 1s/step - accuracy: 0.9435 - loss: 0.1598 - val_accuracy: 0.9518 - val_loss: 0.1400\n",
      "Epoch 6/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2017s\u001b[0m 1s/step - accuracy: 0.9523 - loss: 0.1370 - val_accuracy: 0.9532 - val_loss: 0.1345\n",
      "Epoch 7/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2130s\u001b[0m 2s/step - accuracy: 0.9534 - loss: 0.1294 - val_accuracy: 0.9558 - val_loss: 0.1302\n",
      "Epoch 8/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1889s\u001b[0m 1s/step - accuracy: 0.9573 - loss: 0.1197 - val_accuracy: 0.9555 - val_loss: 0.1278\n",
      "Epoch 9/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2022s\u001b[0m 1s/step - accuracy: 0.9621 - loss: 0.1073 - val_accuracy: 0.9551 - val_loss: 0.1351\n",
      "Epoch 10/10\n",
      "\u001b[1m1358/1358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1759s\u001b[0m 1s/step - accuracy: 0.9644 - loss: 0.1044 - val_accuracy: 0.9554 - val_loss: 0.1336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30a13a360>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pretrained MobileNetV2 Model from Local File\n",
    "weights_path = '/Users/bhaveshreddy/Downloads/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(weights=weights_path, include_top=False, input_shape=(img_size, img_size, 3))\n",
    "base_model.trainable = False  # Freeze the base model layers\n",
    "\n",
    "# Build the Model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6f7ff5-5eb0-42fa-a640-f2fa82e918ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Class Name: Apple___Black_rot\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    img_array = np.array(img).astype('float32') / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Function to Predict the Class of an Image\n",
    "def predict_image_class(model, image_path, class_indices):\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_img)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    class_indices = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "    predicted_class_name = class_indices.get(predicted_class_index, \"Unknown\")\n",
    "    return predicted_class_name\n",
    "\n",
    "# Save class indices to JSON\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "\n",
    "# Load class indices from JSON\n",
    "with open('class_indices.json', 'r') as f:\n",
    "    loaded_class_indices = json.load(f)\n",
    "\n",
    "# Example Usage\n",
    "image_path = '/Users/bhaveshreddy/Desktop/cardio-risk-prediction/checking.JPG'\n",
    "predicted_class_name = predict_image_class(model, image_path, loaded_class_indices)\n",
    "print(\"Predicted Class Name:\", predicted_class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d4bf06-68ca-47c6-bf27-b99ed0f3f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the Model\n",
    "model.save('MobileNetV2-2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45721606-87cb-4950-95df-c11abb351240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
