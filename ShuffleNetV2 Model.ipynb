{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a6ef4f-6f64-402a-839b-e16e26e8492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4da065d-831e-4f13-a681-92b69e3fc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Mixed Precision for Faster Training\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536c3493-93a2-4ff5-adf2-3add15384b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc6cda8-2afb-4d68-8d63-e8faa034807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Apple Metal GPU acceleration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d692954c-cd16-4929-96b2-22bfe2158fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable XLA Optimization\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07292853-d3f2-48bd-82b2-3dcefe1eef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "img_size = 224  # Reduced size for efficiency\n",
    "batch_size = 16  # Reduced batch size to prevent memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6966ffd1-e3b3-471c-add0-92e1032ef249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Path\n",
    "base_dir = '/Users/bhaveshreddy/Desktop/Capstone Project/plantvillage dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27fca54-400e-4d47-8d45-fe3a20c2c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Folders: ['grayscale', '.DS_Store', 'segmented', 'color']\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset folders exist\n",
    "print(\"Dataset Folders:\", os.listdir(base_dir))\n",
    "\n",
    "# Image Data Generators (Optimized augmentations)\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,  # Reduced to avoid over-distortion\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 20% validation split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85737de9-27eb-4c5f-b1cb-4b68a25e9671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43456 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "# Train Generator\n",
    "train_generator = data_gen.flow_from_directory(\n",
    "    os.path.join(base_dir, \"color\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7314cc65-9047-4aed-bddf-6be391b0636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10849 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "# Validation Generator\n",
    "validation_generator = data_gen.flow_from_directory(\n",
    "    os.path.join(base_dir, \"color\"),\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c61ef6-d802-423b-9cfc-77d2f2aac230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ShuffleNetV2 Model\n",
    "def ShuffleNetV2(input_shape=(224, 224, 3), num_classes=10):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(24, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    for _ in range(3):  # Reduce complexity by having fewer stages\n",
    "        x = layers.Conv2D(48, (1, 1), activation='relu')(x)\n",
    "        x = layers.DepthwiseConv2D((3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "        x = layers.Conv2D(96, (1, 1), activation='relu')(x)\n",
    "        x = layers.Add()([x, x])\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f770938-b7a4-47ad-92b0-3c04c9077e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.1963 - loss: 3.0263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2586s\u001b[0m 941ms/step - accuracy: 0.1963 - loss: 3.0262 - val_accuracy: 0.4066 - val_loss: 2.1607 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2482s\u001b[0m 914ms/step - accuracy: 0.4244 - loss: 2.0140 - val_accuracy: 0.6222 - val_loss: 1.2551 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2686s\u001b[0m 989ms/step - accuracy: 0.6236 - loss: 1.2482 - val_accuracy: 0.7228 - val_loss: 0.8733 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2561s\u001b[0m 942ms/step - accuracy: 0.7151 - loss: 0.9103 - val_accuracy: 0.7982 - val_loss: 0.6324 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2288s\u001b[0m 842ms/step - accuracy: 0.7708 - loss: 0.7355 - val_accuracy: 0.8188 - val_loss: 0.5550 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2318s\u001b[0m 853ms/step - accuracy: 0.7934 - loss: 0.6389 - val_accuracy: 0.8506 - val_loss: 0.4546 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2375s\u001b[0m 874ms/step - accuracy: 0.8232 - loss: 0.5471 - val_accuracy: 0.8714 - val_loss: 0.3849 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2466s\u001b[0m 908ms/step - accuracy: 0.8425 - loss: 0.4916 - val_accuracy: 0.8818 - val_loss: 0.3595 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2456s\u001b[0m 904ms/step - accuracy: 0.8550 - loss: 0.4393 - val_accuracy: 0.8951 - val_loss: 0.3238 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2417s\u001b[0m 890ms/step - accuracy: 0.8698 - loss: 0.4033 - val_accuracy: 0.9098 - val_loss: 0.2786 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2458s\u001b[0m 905ms/step - accuracy: 0.8868 - loss: 0.3582 - val_accuracy: 0.9061 - val_loss: 0.2870 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2443s\u001b[0m 899ms/step - accuracy: 0.8910 - loss: 0.3381 - val_accuracy: 0.9138 - val_loss: 0.2610 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2507s\u001b[0m 923ms/step - accuracy: 0.8975 - loss: 0.3143 - val_accuracy: 0.9380 - val_loss: 0.1875 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2471s\u001b[0m 910ms/step - accuracy: 0.9078 - loss: 0.2887 - val_accuracy: 0.9518 - val_loss: 0.1596 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2445s\u001b[0m 900ms/step - accuracy: 0.9147 - loss: 0.2560 - val_accuracy: 0.9362 - val_loss: 0.2013 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d09ab40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Model\n",
    "num_classes = len(train_generator.class_indices)\n",
    "model = ShuffleNetV2(input_shape=(img_size, img_size, 3), num_classes=num_classes)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "# Train the Model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping, lr_scheduler, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cdde0aa-00be-4824-8d9e-552bd2f4ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load and Preprocess the Image\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    img_array = np.array(img).astype('float32') / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand batch dimension\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e0212e2-73db-45f5-bc13-affffe431462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Predict the Class of an Image\n",
    "def predict_image_class(model, image_path, class_indices):\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_img)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    class_indices = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "    predicted_class_name = class_indices.get(predicted_class_index, \"Unknown\")\n",
    "    return predicted_class_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33048966-91de-4203-a170-f66d2979556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class indices to JSON\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "\n",
    "# Load class indices from JSON\n",
    "with open('class_indices.json', 'r') as f:\n",
    "    loaded_class_indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c2406d-8495-4444-9696-e344b36c022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "Predicted Class Name: Apple___Black_rot\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "image_path = '/Users/bhaveshreddy/Desktop/cardio-risk-prediction/checking.JPG'\n",
    "predicted_class_name = predict_image_class(model, image_path, loaded_class_indices)\n",
    "print(\"Predicted Class Name:\", predicted_class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "528cc5dc-a2ba-44d2-80ad-23ee3bd6041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /Users/bhaveshreddy/Desktop/saved/plant_disease_model.keras\n",
      "Model saved at: /Users/bhaveshreddy/Desktop/saved/plant_disease_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the save path for .h5 and .keras\n",
    "save_dir = '/Users/bhaveshreddy/Desktop/saved'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the model in both formats\n",
    "model.save(os.path.join(save_dir, 'plant_disease_model.keras'))\n",
    "model.save(os.path.join(save_dir, 'plant_disease_model.h5'))\n",
    "\n",
    "print(f\"Model saved at: {save_dir}/plant_disease_model.keras\")\n",
    "print(f\"Model saved at: {save_dir}/plant_disease_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2368a-9b78-4e74-b9aa-f2020c2f1d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
